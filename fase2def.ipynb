{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87cb46ed9761fb5c",
   "metadata": {},
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastFM import sgd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import sparse\n",
    "\n",
    "df = pd.read_csv(\"dataset_final.csv\")\n",
    "\n",
    "X = df[['temporal_1', 'temporal_2', 'temporal_3']]\n",
    "y = df['rating']\n",
    "\n",
    "# üîπ 3. Normalizar features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convertir a sparse matrix (porque fastFM necesita sparse)\n",
    "X_sparse = sparse.csr_matrix(X_scaled)\n",
    "\n",
    "# üîπ 4. Separar en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sparse, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# üîπ 5. Crear el modelo Factorization Machine\n",
    "fm = sgd.FMRegression(n_iter=1000, init_stdev=0.1, l2_reg_w=0.1, l2_reg_V=0.5, rank=8, random_state=42)\n",
    "\n",
    "# üîπ 6. Entrenar el modelo\n",
    "fm.fit(X_train, y_train)\n",
    "\n",
    "# üîπ 7. Predecir\n",
    "y_pred = fm.predict(X_test)\n",
    "\n",
    "# üîπ 8. Evaluar con RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"\\nüîç RMSE obtenido en test: {rmse:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0d67e1efd90be",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92fb53d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Masking, TimeDistributed, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# === 1. Cargar dataset ===\n",
    "df = pd.read_csv(\"dataset_final_3.csv\")\n",
    "\n",
    "# === 1A. Oversampling de ratings ‚â§ 2.0 ===\n",
    "df_low = df[df['rating'] <= 2.0]\n",
    "factor = int(np.ceil(df['rating'].value_counts().max() / len(df_low)))\n",
    "\n",
    "# Concatenar varias veces y barajar\n",
    "df = pd.concat([df] + [df_low] * factor, ignore_index=True)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# === 2. Features usadas ===\n",
    "temporal_features = [\n",
    "    'temporal_1', 'temporal_2', 'temporal_3',\n",
    "    'is_weekend', 'season_encoded',  # estos no se escalan\n",
    "    'year', 'month', 'weekday',\n",
    "    'consumo_semanal_usuario',\n",
    "    'interactions_user', 'interactions_movie',\n",
    "    'days_since_last_user_interaction'\n",
    "]\n",
    "\n",
    "\n",
    "# === 3. Codificar userId y movieId ===\n",
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "\n",
    "df['user_id_enc'] = user_encoder.fit_transform(df['userId'])\n",
    "df['movie_id_enc'] = movie_encoder.fit_transform(df['movieId'])\n",
    "\n",
    "n_users = df['user_id_enc'].nunique()\n",
    "n_movies = df['movie_id_enc'].nunique()\n",
    "\n",
    "# === 4. Agrupar en secuencias ===\n",
    "user_seqs = defaultdict(list)\n",
    "movie_seqs = defaultdict(list)\n",
    "rating_seqs = defaultdict(list)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    uid = row['user_id_enc']\n",
    "    mid = row['movie_id_enc']\n",
    "    x = row[temporal_features].values.astype('float32')\n",
    "    r = row['rating']\n",
    "    user_seqs[uid].append(x)\n",
    "    movie_seqs[uid].append(mid)\n",
    "    rating_seqs[uid].append(r)\n",
    "\n",
    "# === 5. Convertir a listas ===\n",
    "X_seq = list(user_seqs.values())\n",
    "movie_seq = list(movie_seqs.values())\n",
    "y_seq = list(rating_seqs.values())\n",
    "\n",
    "# === 6. Padding ===\n",
    "maxlen = 50\n",
    "X_padded = pad_sequences(X_seq, maxlen=maxlen, padding='pre', dtype='float32')\n",
    "movie_padded = pad_sequences(movie_seq, maxlen=maxlen, padding='pre')\n",
    "y_padded = pad_sequences(y_seq, maxlen=maxlen, padding='pre', dtype='float32')\n",
    "user_ids = np.array(list(user_seqs.keys()))\n",
    "\n",
    "# === 7. Separar en train/test ===\n",
    "X_tr, X_te, M_tr, M_te, U_tr, U_te, Y_tr, Y_te = train_test_split(\n",
    "    X_padded, movie_padded, user_ids, y_padded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === 8. Modelo con embeddings ===\n",
    "input_seq = Input(shape=(maxlen, X_tr.shape[2]), name='features_input')\n",
    "input_uid = Input(shape=(1,), name='user_input')\n",
    "input_mid = Input(shape=(maxlen,), name='movie_input')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "714d8303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " user_input (InputLayer)     [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " user_embedding (Embedding)  (None, 1, 16)                3206144   ['user_input[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 1, 16)                0         ['user_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)         (None, 16)                   0         ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " movie_input (InputLayer)    [(None, 50)]                 0         []                            \n",
      "                                                                                                  \n",
      " features_input (InputLayer  [(None, 50, 12)]             0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " repeat_vector_3 (RepeatVec  (None, 50, 16)               0         ['reshape_4[0][0]']           \n",
      " tor)                                                                                             \n",
      "                                                                                                  \n",
      " movie_embedding (Embedding  (None, 50, 16)               814064    ['movie_input[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 50, 44)               0         ['features_input[0][0]',      \n",
      " )                                                                   'repeat_vector_3[0][0]',     \n",
      "                                                                     'movie_embedding[0][0]']     \n",
      "                                                                                                  \n",
      " masking_3 (Masking)         (None, 50, 44)               0         ['concatenate_3[0][0]']       \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               (None, 50, 64)               27904     ['masking_3[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 50, 64)               0         ['lstm_3[0][0]']              \n",
      "                                                                                                  \n",
      " time_distributed_3 (TimeDi  (None, 50, 1)                65        ['dropout_8[0][0]']           \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)           (None, 50, 1)                0         ['time_distributed_3[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4048177 (15.44 MB)\n",
      "Trainable params: 4048177 (15.44 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/25\n",
      "1253/1253 [==============================] - 46s 33ms/step - loss: 0.3724 - mae: 0.7631 - mse: 0.9409 - val_loss: 0.3124 - val_mae: 0.6939 - val_mse: 0.7615\n",
      "Epoch 2/25\n",
      "1253/1253 [==============================] - 18s 14ms/step - loss: 0.2980 - mae: 0.6773 - mse: 0.7188 - val_loss: 0.3103 - val_mae: 0.6914 - val_mse: 0.7540\n",
      "Epoch 3/25\n",
      "1253/1253 [==============================] - 16s 13ms/step - loss: 0.2710 - mae: 0.6455 - mse: 0.6391 - val_loss: 0.3041 - val_mae: 0.6842 - val_mse: 0.7341\n",
      "Epoch 4/25\n",
      "1253/1253 [==============================] - 15s 12ms/step - loss: 0.2542 - mae: 0.6247 - mse: 0.5954 - val_loss: 0.3058 - val_mae: 0.6857 - val_mse: 0.7430\n",
      "Epoch 5/25\n",
      "1253/1253 [==============================] - 14s 11ms/step - loss: 0.2454 - mae: 0.6135 - mse: 0.5726 - val_loss: 0.3044 - val_mae: 0.6840 - val_mse: 0.7398\n",
      "Epoch 6/25\n",
      "1253/1253 [==============================] - 13s 10ms/step - loss: 0.2360 - mae: 0.6012 - mse: 0.5484 - val_loss: 0.3039 - val_mae: 0.6836 - val_mse: 0.7370\n",
      "Epoch 7/25\n",
      "1253/1253 [==============================] - 13s 10ms/step - loss: 0.2308 - mae: 0.5942 - mse: 0.5348 - val_loss: 0.3044 - val_mae: 0.6841 - val_mse: 0.7396\n",
      "Epoch 8/25\n",
      "1253/1253 [==============================] - 13s 10ms/step - loss: 0.2270 - mae: 0.5891 - mse: 0.5248 - val_loss: 0.3036 - val_mae: 0.6834 - val_mse: 0.7357\n",
      "Epoch 9/25\n",
      "1253/1253 [==============================] - 14s 12ms/step - loss: 0.2238 - mae: 0.5848 - mse: 0.5164 - val_loss: 0.3030 - val_mae: 0.6821 - val_mse: 0.7375\n",
      "Epoch 10/25\n",
      "1253/1253 [==============================] - 14s 11ms/step - loss: 0.2208 - mae: 0.5808 - mse: 0.5087 - val_loss: 0.3025 - val_mae: 0.6822 - val_mse: 0.7318\n",
      "Epoch 11/25\n",
      "1253/1253 [==============================] - 13s 10ms/step - loss: 0.2183 - mae: 0.5774 - mse: 0.5019 - val_loss: 0.3020 - val_mae: 0.6812 - val_mse: 0.7328\n",
      "Epoch 12/25\n",
      "1253/1253 [==============================] - 13s 10ms/step - loss: 0.2159 - mae: 0.5743 - mse: 0.4954 - val_loss: 0.3012 - val_mae: 0.6806 - val_mse: 0.7295\n",
      "Epoch 13/25\n",
      "1253/1253 [==============================] - 13s 10ms/step - loss: 0.2135 - mae: 0.5711 - mse: 0.4889 - val_loss: 0.3004 - val_mae: 0.6796 - val_mse: 0.7269\n",
      "Epoch 14/25\n",
      "1253/1253 [==============================] - 12s 10ms/step - loss: 0.2112 - mae: 0.5681 - mse: 0.4827 - val_loss: 0.3009 - val_mae: 0.6797 - val_mse: 0.7318\n",
      "Epoch 15/25\n",
      "1253/1253 [==============================] - 13s 10ms/step - loss: 0.2087 - mae: 0.5649 - mse: 0.4759 - val_loss: 0.3006 - val_mae: 0.6795 - val_mse: 0.7295\n",
      "Epoch 16/25\n",
      "1253/1253 [==============================] - 14s 11ms/step - loss: 0.2061 - mae: 0.5615 - mse: 0.4688 - val_loss: 0.2995 - val_mae: 0.6786 - val_mse: 0.7238\n",
      "Epoch 17/25\n",
      "1253/1253 [==============================] - 14s 11ms/step - loss: 0.2037 - mae: 0.5583 - mse: 0.4622 - val_loss: 0.2999 - val_mae: 0.6789 - val_mse: 0.7265\n",
      "Epoch 18/25\n",
      "1253/1253 [==============================] - 13s 11ms/step - loss: 0.2014 - mae: 0.5552 - mse: 0.4560 - val_loss: 0.2998 - val_mae: 0.6791 - val_mse: 0.7235\n",
      "Epoch 19/25\n",
      "1253/1253 [==============================] - 12s 10ms/step - loss: 0.1993 - mae: 0.5524 - mse: 0.4503 - val_loss: 0.2996 - val_mae: 0.6789 - val_mse: 0.7224\n",
      "Epoch 20/25\n",
      "1253/1253 [==============================] - 12s 10ms/step - loss: 0.1974 - mae: 0.5498 - mse: 0.4451 - val_loss: 0.2991 - val_mae: 0.6782 - val_mse: 0.7218\n",
      "Epoch 21/25\n",
      "1253/1253 [==============================] - 12s 10ms/step - loss: 0.1956 - mae: 0.5475 - mse: 0.4403 - val_loss: 0.2995 - val_mae: 0.6790 - val_mse: 0.7211\n",
      "Epoch 22/25\n",
      "1253/1253 [==============================] - 12s 10ms/step - loss: 0.1940 - mae: 0.5454 - mse: 0.4360 - val_loss: 0.2996 - val_mae: 0.6785 - val_mse: 0.7244\n",
      "Epoch 23/25\n",
      "1253/1253 [==============================] - 12s 10ms/step - loss: 0.1924 - mae: 0.5433 - mse: 0.4317 - val_loss: 0.2996 - val_mae: 0.6786 - val_mse: 0.7244\n",
      "Epoch 24/25\n",
      "1253/1253 [==============================] - 12s 10ms/step - loss: 0.1910 - mae: 0.5414 - mse: 0.4279 - val_loss: 0.3001 - val_mae: 0.6788 - val_mse: 0.7298\n",
      "Epoch 25/25\n",
      "1253/1253 [==============================] - 12s 10ms/step - loss: 0.1898 - mae: 0.5397 - mse: 0.4245 - val_loss: 0.3004 - val_mae: 0.6797 - val_mse: 0.7255\n",
      "1253/1253 [==============================] - 4s 3ms/step\n",
      "\n",
      "‚úÖ RMSE: 0.8496\n",
      "‚úÖ MAE: 0.6782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduardo/PycharmProjects/Calculo_Entropia/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Embedding usuario (igual en todos los pasos ‚Üí expandimos)\n",
    "import tensorflow as tf\n",
    "user_emb = Embedding(input_dim=n_users, output_dim=16, name='user_embedding')(input_uid)\n",
    "user_emb = Dropout(0.2)(user_emb)\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "user_emb = Reshape((16,))(user_emb)  # convierte (None, 1, 16) ‚Üí (None, 16)\n",
    "user_emb = tf.keras.layers.RepeatVector(maxlen)(user_emb)  # ahora s√≠ funciona\n",
    "\n",
    "# Embedding por pel√≠cula por paso\n",
    "movie_emb = Embedding(input_dim=n_movies, output_dim=16, name='movie_embedding')(input_mid)  # shape: (batch, maxlen, 16)\n",
    "\n",
    "# Concatenar embeddings + features\n",
    "x_concat = Concatenate(axis=-1)([input_seq, user_emb, movie_emb])\n",
    "\n",
    "# Red LSTM\n",
    "x = Masking(mask_value=0.0)(x_concat)\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = Dropout(0.4)(x)\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "output = TimeDistributed(Dense(1, activation='sigmoid'))(x)\n",
    "output = Lambda(lambda y: y * 4.5 + 0.5)(output)  # mapea [0,1] -> [0.5, 5.0]\n",
    "# Modelo final\n",
    "model = Model(inputs=[input_seq, input_uid, input_mid], outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='huber', metrics=['mae', 'mse'])\n",
    "model.summary()\n",
    "\n",
    "# === 9. Entrenamiento ===\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_tr, U_tr, M_tr],\n",
    "    Y_tr[..., np.newaxis],  # expandir para (batch, steps, 1)\n",
    "    validation_data=([X_te, U_te, M_te], Y_te[..., np.newaxis]),\n",
    "    epochs=25,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "import joblib\n",
    "\n",
    "# Luego de hacer fit\n",
    "user_encoder = LabelEncoder()\n",
    "user_encoder.fit(df['userId'])\n",
    "\n",
    "movie_encoder = LabelEncoder()\n",
    "movie_encoder.fit(df['movieId'])\n",
    "\n",
    "# Guardar para usar en predicci√≥n despu√©s\n",
    "joblib.dump(user_encoder, \"user_encoder.pkl\")\n",
    "joblib.dump(movie_encoder, \"movie_encoder.pkl\")\n",
    "\n",
    "# === 10. Evaluaci√≥n ===\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "y_pred = model.predict([X_te, U_te, M_te]).squeeze()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(Y_te.flatten(), y_pred.flatten()))\n",
    "mae = mean_absolute_error(Y_te.flatten(), y_pred.flatten())\n",
    "\n",
    "print(f\"\\n‚úÖ RMSE: {rmse:.4f}\")\n",
    "print(f\"‚úÖ MAE: {mae:.4f}\")\n",
    "\n",
    "# === 11. Guardar modelo ===\n",
    "model.save(\"rnn_model_with_embeddings4.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e1b34",
   "metadata": {},
   "source": [
    "## Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcf7ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_test(df_new_raw, user_encoder, movie_encoder, scaler, reducer):\n",
    "    import holidays\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    df = df_new_raw.copy()\n",
    "\n",
    "    # --- Codificaci√≥n segura para userId y movieId ---\n",
    "    known_users = set(user_encoder.classes_)\n",
    "    known_movies = set(movie_encoder.classes_)\n",
    "\n",
    "    df['userId'] = df['userId'].apply(lambda x: x if x in known_users else -1)\n",
    "    df['movieId'] = df['movieId'].apply(lambda x: x if x in known_movies else -1)\n",
    "\n",
    "    if -1 not in user_encoder.classes_:\n",
    "        user_encoder.classes_ = np.append(user_encoder.classes_, -1)\n",
    "    if -1 not in movie_encoder.classes_:\n",
    "        movie_encoder.classes_ = np.append(movie_encoder.classes_, -1)\n",
    "\n",
    "    df['user_id_enc'] = user_encoder.transform(df['userId'])\n",
    "    df['movie_id_enc'] = movie_encoder.transform(df['movieId'])\n",
    "\n",
    "    # --- Temporal features ---\n",
    "    df['date'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['weekday'] = df['date'].dt.weekday\n",
    "    df['is_weekend'] = df['weekday'].isin([5,6]).astype(int)\n",
    "\n",
    "    def get_season(month):\n",
    "        if month in [12, 1, 2]: return 'invierno'\n",
    "        elif month in [3, 4, 5]: return 'primavera'\n",
    "        elif month in [6, 7, 8]: return 'verano'\n",
    "        else: return 'oto√±o'\n",
    "\n",
    "    df['season'] = df['month'].apply(get_season)\n",
    "    df['season_encoded'] = df['season'].map({'invierno':0, 'primavera':1, 'verano':2, 'oto√±o':3})\n",
    "    df['is_holiday'] = df['date'].isin(holidays.Ecuador(years=range(df['year'].min(), df['year'].max()+1))).astype(int)\n",
    "\n",
    "    # --- Consumo semanal ---\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "    consumo = df.groupby(['userId', 'week_of_year']).size().reset_index(name='consumo_semanal_usuario')\n",
    "    df = pd.merge(df, consumo, on=['userId', 'week_of_year'], how='left')\n",
    "\n",
    "    # --- UMAP features ---\n",
    "    umap_input = scaler.transform(df[['year', 'month', 'weekday', 'season_encoded', 'is_weekend', 'is_holiday']])\n",
    "    umap_result = reducer.transform(umap_input)\n",
    "    df[['temporal_1', 'temporal_2', 'temporal_3']] = umap_result\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99732a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_240349/1363179351.py:48: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
      "  df_new['is_holiday'] = df_new['date'].isin(holidays.Ecuador(\n",
      "/home/eduardo/PycharmProjects/Calculo_Entropia/.venv/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "import holidays\n",
    "\n",
    "# === Cargar modelos previos ===\n",
    "user_encoder = joblib.load(\"user_encoder.pkl\")\n",
    "movie_encoder = joblib.load(\"movie_encoder.pkl\")\n",
    "scaler_umap = joblib.load(\"scaler_temporal2.pkl\")\n",
    "reducer = joblib.load(\"umap_model2.pkl\")\n",
    "scaler_feats = joblib.load(\"scaler_features_rnn.pkl\")\n",
    "\n",
    "# === Cargar test ===\n",
    "df_new = pd.read_csv(\"testfinal.csv\")\n",
    "\n",
    "# === Codificaci√≥n segura de IDs ===\n",
    "known_users = set(user_encoder.classes_)\n",
    "known_movies = set(movie_encoder.classes_)\n",
    "\n",
    "df_new['userId'] = df_new['userId'].apply(lambda x: x if x in known_users else -1)\n",
    "df_new['movieId'] = df_new['movieId'].apply(lambda x: x if x in known_movies else -1)\n",
    "\n",
    "if -1 not in user_encoder.classes_:\n",
    "    user_encoder.classes_ = np.append(user_encoder.classes_, -1)\n",
    "if -1 not in movie_encoder.classes_:\n",
    "    movie_encoder.classes_ = np.append(movie_encoder.classes_, -1)\n",
    "\n",
    "df_new['user_id_enc'] = user_encoder.transform(df_new['userId'])\n",
    "df_new['movie_id_enc'] = movie_encoder.transform(df_new['movieId'])\n",
    "\n",
    "# === Features temporales ===\n",
    "df_new['date'] = pd.to_datetime(df_new['timestamp'], unit='s')\n",
    "df_new['year'] = df_new['date'].dt.year\n",
    "df_new['month'] = df_new['date'].dt.month\n",
    "df_new['weekday'] = df_new['date'].dt.weekday\n",
    "df_new['is_weekend'] = df_new['weekday'].isin([5, 6]).astype(int)\n",
    "df_new['week_of_year'] = df_new['date'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "df_new['season'] = df_new['month'].apply(\n",
    "    lambda m: 'invierno' if m in [12, 1, 2] else\n",
    "              'primavera' if m in [3, 4, 5] else\n",
    "              'verano' if m in [6, 7, 8] else 'oto√±o'\n",
    ")\n",
    "df_new['season_encoded'] = df_new['season'].map({'invierno': 0, 'primavera': 1, 'verano': 2, 'oto√±o': 3})\n",
    "df_new['is_holiday'] = df_new['date'].isin(holidays.Ecuador(\n",
    "    years=range(df_new['year'].min(), df_new['year'].max() + 1)\n",
    ")).astype(int)\n",
    "\n",
    "# === Consumo semanal ===\n",
    "consumo = df_new.groupby(['userId', 'week_of_year']).size().reset_index(name='consumo_semanal_usuario')\n",
    "df_new = pd.merge(df_new, consumo, on=['userId', 'week_of_year'], how='left')\n",
    "\n",
    "# === UMAP features ===\n",
    "umap_input = scaler_umap.transform(df_new[['year', 'month', 'weekday', 'season_encoded', 'is_weekend', 'is_holiday']])\n",
    "umap_result = reducer.transform(umap_input)\n",
    "df_new[['temporal_1', 'temporal_2', 'temporal_3']] = umap_result\n",
    "\n",
    "# === Interacciones y tiempo desde √∫ltima interacci√≥n ===\n",
    "df_new['interactions_user'] = df_new.groupby('userId').cumcount() + 1\n",
    "df_new['interactions_movie'] = df_new.groupby('movieId').cumcount() + 1\n",
    "\n",
    "df_new['last_timestamp'] = df_new.groupby('userId')['timestamp'].shift(1)\n",
    "df_new['days_since_last_user_interaction'] = (df_new['timestamp'] - df_new['last_timestamp']) / (60 * 60 * 24)\n",
    "df_new['days_since_last_user_interaction'] = df_new['days_since_last_user_interaction'].fillna(0)\n",
    "\n",
    "# === Estandarizar variables ===\n",
    "features_to_scale = [\n",
    "    'year', 'month', 'weekday',\n",
    "    'interactions_user', 'interactions_movie',\n",
    "    'days_since_last_user_interaction',\n",
    "    'consumo_semanal_usuario'\n",
    "]\n",
    "df_new[features_to_scale] = scaler_feats.transform(df_new[features_to_scale])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d6b73f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Features finales para input ---\n",
    "temporal_features = [\n",
    "    'temporal_1', 'temporal_2', 'temporal_3',\n",
    "    'is_weekend', 'season_encoded',\n",
    "    'year', 'month', 'weekday',\n",
    "    'consumo_semanal_usuario',\n",
    "    'interactions_user', 'interactions_movie', 'days_since_last_user_interaction'\n",
    "]\n",
    "\n",
    "# --- Agrupar en secuencias ---\n",
    "user_sequences = defaultdict(list)\n",
    "movie_sequences = defaultdict(list)\n",
    "\n",
    "for _, row in df_new.iterrows():\n",
    "    uid = row['user_id_enc']\n",
    "    mid = row['movie_id_enc']\n",
    "    x = row[temporal_features].values.astype('float32')\n",
    "    user_sequences[uid].append(x)\n",
    "    movie_sequences[uid].append(mid)\n",
    "\n",
    "# --- Padding ---\n",
    "X_new = list(user_sequences.values())\n",
    "M_new = list(movie_sequences.values())\n",
    "U_new = np.array(list(user_sequences.keys()))\n",
    "\n",
    "maxlen = 50\n",
    "X_new_padded = pad_sequences(X_new, maxlen=maxlen, padding='pre', dtype='float32')\n",
    "M_new_padded = pad_sequences(M_new, maxlen=maxlen, padding='pre')\n",
    "U_new_padded = U_new.reshape(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbf34816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5274/5274 [==============================] - 17s 3ms/step\n",
      "‚úÖ Predicciones realizadas:\n",
      "    userId  predicted_last_rating\n",
      "0    50403                    3.7\n",
      "1    32443                    4.6\n",
      "2   188231                    3.3\n",
      "3    54812                    4.1\n",
      "4    32676                    3.8\n",
      "5    89328                    1.2\n",
      "6    19325                    1.1\n",
      "7   150689                    1.7\n",
      "8   115166                    1.8\n",
      "9    99558                    3.6\n",
      "10  105746                    1.8\n",
      "11  173703                    4.5\n",
      "12   28872                    3.6\n",
      "13  179715                    2.5\n",
      "14  177642                    4.5\n",
      "15  179218                    1.3\n",
      "16  151556                    4.1\n",
      "17   31053                    4.9\n",
      "18  120287                    4.7\n",
      "19   49000                    4.4\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Cargar modelo ===\n",
    "model = load_model(\"rnn_model_with_embeddings4.h5\")\n",
    "\n",
    "# === Predecir secuencia ===\n",
    "y_pred_seq = model.predict([X_new_padded, U_new_padded, M_new_padded]).squeeze()\n",
    "\n",
    "# === Obtener √∫ltima predicci√≥n v√°lida por usuario ===\n",
    "y_pred_last = []\n",
    "for seq_pred, seq_input in zip(y_pred_seq, X_new_padded):\n",
    "    mask = np.any(seq_input != 0, axis=1)\n",
    "    last_valid_idx = np.where(mask)[0][-1]\n",
    "    y_pred_last.append(seq_pred[last_valid_idx])\n",
    "\n",
    "# === Reconstruir userId de forma segura ===\n",
    "# Validar que todos los valores de U_new est√©n dentro del rango v√°lido del encoder\n",
    "valid_range = len(user_encoder.classes_)\n",
    "user_ids_decoded = []\n",
    "\n",
    "for u in U_new.flatten():\n",
    "    if u < valid_range:\n",
    "        user_ids_decoded.append(user_encoder.inverse_transform([u])[0])\n",
    "    else:\n",
    "        user_ids_decoded.append(\"unknown\")\n",
    "\n",
    "# === Resultados finales ===\n",
    "results = pd.DataFrame({\n",
    "    'userId': user_ids_decoded,\n",
    "    'predicted_last_rating': np.round(y_pred_last, 1)\n",
    "})\n",
    "\n",
    "# === Exportar o mostrar ===\n",
    "print(\"‚úÖ Predicciones realizadas:\")\n",
    "print(results.head(20))\n",
    "results.to_csv(\"predicciones_rnn_embeddings.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c427aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Predicciones redondeadas:\n",
      "    userId  predicted_last_rating\n",
      "0    50403                    3.5\n",
      "1    32443                    4.5\n",
      "2   188231                    3.5\n",
      "3    54812                    4.0\n",
      "4    32676                    4.0\n",
      "5    89328                    1.0\n",
      "6    19325                    1.0\n",
      "7   150689                    1.5\n",
      "8   115166                    2.0\n",
      "9    99558                    3.5\n",
      "10  105746                    2.0\n",
      "11  173703                    4.5\n",
      "12   28872                    3.5\n",
      "13  179715                    2.5\n",
      "14  177642                    4.5\n",
      "15  179218                    1.5\n",
      "16  151556                    4.0\n",
      "17   31053                    5.0\n",
      "18  120287                    4.5\n",
      "19   49000                    4.5\n"
     ]
    }
   ],
   "source": [
    "results['predicted_last_rating'] = np.round(results['predicted_last_rating'] * 2) / 2\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"‚úÖ Predicciones redondeadas:\")\n",
    "print(results.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d0905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "calculo_entropia",
   "language": "python",
   "name": "calculo_entropia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
